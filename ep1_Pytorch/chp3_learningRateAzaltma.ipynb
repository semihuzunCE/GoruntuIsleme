{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from skimage import io\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class veri(Dataset): \n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.annotations=pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    def __getitem__(self, index):\n",
    "       img_path=os.path.join(self.root_dir,self.annotations.iloc[index,0])\n",
    "       image=io.imread(img_path)\n",
    "       y_label=torch.tensor(int(self.annotations.iloc[index,1]))\n",
    "       \n",
    "       if self.transform:\n",
    "              image=self.transform(image)\n",
    "              return (image,y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=veri(csv_file=\"D:\\\\udemy_data\\\\f1_classification\\\\f111.csv\",root_dir=\"D:\\\\udemy_data\\\\f1_classification\",\n",
    "transform=torchvision.transforms.Compose([ \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(size=(28,28)),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set,test_set=torch.utils.data.random_split(dataset,[200,79])\n",
    "train_loader=DataLoader(dataset=train_set,batch_size=1,shuffle=False)\n",
    "test_loader=DataLoader(dataset=test_set,batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        #conv katmanları:\n",
    "        self.conv1=nn.Conv2d(in_channels=3,out_channels=4,kernel_size=(5,5)) \n",
    "        self.conv2=nn.Conv2d(in_channels=4,out_channels=8,kernel_size=(3,3))\n",
    "        self.conv3=nn.Conv2d(in_channels=8,out_channels=16,kernel_size=(2,2))\n",
    "        self.conv4=nn.Conv2d(in_channels=16,out_channels=32,kernel_size=(2,2))\n",
    "        #max pooling katmanı:\n",
    "\n",
    "        self.max=nn.MaxPool2d(kernel_size=(2,2)) \n",
    "\n",
    "       \n",
    "        self.func=nn.ELU()\n",
    "\n",
    "       \n",
    "        self.fc1=nn.Linear(in_features=32,out_features=50)\n",
    "        self.fc2=nn.Linear(in_features=50,out_features=50)\n",
    "        self.fc3=nn.Linear(in_features=50,out_features=100)\n",
    "        self.fc4=nn.Linear(in_features=100,out_features=4)\n",
    "\n",
    "    def forward(self,x):\n",
    "         x=self.conv1(x) \n",
    "         x=self.func(x)\n",
    "         x=self.max(x)\n",
    "\n",
    "         x=self.conv2(x)\n",
    "         x=self.func(x)\n",
    "         x=self.max(x)\n",
    "\n",
    "         x=self.conv3(x)\n",
    "         x=self.func(x)\n",
    "         x=self.max(x)\n",
    "\n",
    "         x=self.conv4(x)\n",
    "         x=self.func(x)\n",
    "\n",
    "         x=x.view(x.size(0),-1) \n",
    "\n",
    "         x=self.fc1(x)\n",
    "         x=self.func(x)\n",
    "         x=self.fc2(x)\n",
    "         x=self.func(x)\n",
    "         x=self.fc3(x)\n",
    "         x=self.func(x)\n",
    "         x=self.fc4(x)\n",
    "            \n",
    "         return x\n",
    "          \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 LR: [0.001]\n",
      "Iteration: 100 loss: 1.5540134906768799 accuracy: 27.85% Error: 72.15%\n",
      "Iteration: 200 loss: 1.0247942209243774 accuracy: 39.24% Error: 60.76%\n",
      "Epoch: 10 LR: [0.00049]\n",
      "Iteration: 300 loss: 0.201642245054245 accuracy: 46.84% Error: 53.16%\n",
      "Iteration: 400 loss: 0.890767514705658 accuracy: 51.90% Error: 48.10%\n",
      "Epoch: 10 LR: [0.0007]\n",
      "Iteration: 500 loss: 0.04077068716287613 accuracy: 50.63% Error: 49.37%\n",
      "Iteration: 600 loss: 0.5381302833557129 accuracy: 62.03% Error: 37.97%\n",
      "Epoch: 10 LR: [0.000343]\n",
      "Iteration: 700 loss: 0.020208656787872314 accuracy: 62.03% Error: 37.97%\n",
      "Iteration: 800 loss: 0.2673063278198242 accuracy: 65.82% Error: 34.18%\n",
      "Epoch: 10 LR: [0.00049]\n",
      "Iteration: 900 loss: 0.006830438040196896 accuracy: 64.56% Error: 35.44%\n",
      "Iteration: 1000 loss: 0.21239106357097626 accuracy: 67.09% Error: 32.91%\n",
      "Epoch: 10 LR: [0.00024009999999999998]\n",
      "Iteration: 1100 loss: 0.0084925452247262 accuracy: 70.89% Error: 29.11%\n",
      "Iteration: 1200 loss: 0.29906705021858215 accuracy: 68.35% Error: 31.65%\n",
      "Epoch: 10 LR: [0.000343]\n",
      "Iteration: 1300 loss: 0.006089822389185429 accuracy: 72.15% Error: 27.85%\n",
      "Iteration: 1400 loss: 0.29950618743896484 accuracy: 69.62% Error: 30.38%\n",
      "Epoch: 10 LR: [0.00016806999999999998]\n",
      "Iteration: 1500 loss: 0.010910380631685257 accuracy: 73.42% Error: 26.58%\n",
      "Iteration: 1600 loss: 0.40148186683654785 accuracy: 70.89% Error: 29.11%\n",
      "Epoch: 10 LR: [0.00024009999999999998]\n",
      "Iteration: 1700 loss: 0.009197721257805824 accuracy: 73.42% Error: 26.58%\n",
      "Iteration: 1800 loss: 0.38199910521507263 accuracy: 74.68% Error: 25.32%\n",
      "Epoch: 10 LR: [0.00011764899999999998]\n",
      "Iteration: 1900 loss: 0.01475340686738491 accuracy: 73.42% Error: 26.58%\n",
      "Iteration: 2000 loss: 0.33504486083984375 accuracy: 73.42% Error: 26.58%\n",
      "süre: 189.30666494369507\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "start=time.time() \n",
    "\n",
    "model=Net() \n",
    "\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=0.001) \n",
    "\n",
    "error=torch.nn.CrossEntropyLoss() \n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR # kademeli lr azaltması için gerekli fonksiyonu import ettik\n",
    "lr=StepLR(optimizer,step_size=2,gamma=0.7) #optimizer optimizasyon uygulammızı tanımladıgımız değişkenimizdir, step_size kaç epocta bir lr azaltması yapıcagımız ve gamma ize lr azaltma miktarımızdır\n",
    "\n",
    "epoch = 10\n",
    "\n",
    "kayıp=[] \n",
    "count=0\n",
    "iterasyon=[] \n",
    "for i in range(epoch):\n",
    "\n",
    "    lr.step() # bu fonksiyonu modelimizi eğittimiz kısmın başına ekleyerek belirtilen epoch değerlerinde kademeli lr azaltması yapıcaz\n",
    "\n",
    "    print(\"Epoch:\",epoch,\"LR:\",lr.get_lr())\n",
    "\n",
    "    for i,(images,label) in enumerate(train_loader):\n",
    "        tahmin=model(images)\n",
    "        optimizer.zero_grad()\n",
    "        loss=error(tahmin,label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        count+=1\n",
    "\n",
    "        if count % 100 == 0:\n",
    "            total=0\n",
    "            correct=0\n",
    "            correct_hata=0\n",
    "            for image,labels in test_loader:\n",
    "                out=model(image)\n",
    "                pred=torch.max(out.data,1)[1]\n",
    "                total+=len(label)\n",
    "\n",
    "                correct+=(pred==labels).sum() \n",
    "                correct_hata+=(pred!=labels).sum() \n",
    "            dogruluk=100*correct/float(total)\n",
    "            hata=100*correct_hata/float(total)\n",
    "\n",
    "            kayıp.append(loss.data)\n",
    "            iterasyon.append(count)\n",
    "\n",
    "        if count % 100 == 0:\n",
    "            print('Iteration: {} loss: {} accuracy: {:.2f}% Error: {:.2f}%'.format(count,loss.data,dogruluk,hata))\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "end=time.time()\n",
    "print(\"süre:\",end-start)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6890cd249af4e4ffbb6e94abfb53100042eb61c81d00e1f1ac790c73285b1fdf"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
